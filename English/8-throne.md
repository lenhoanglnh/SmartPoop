# 8. On the throne

In front of a full stadium, with cameras from all over the world focused on her, Katia enters the stage under the ovations of the audience, like a rock star. This year, SmartPoop has put a lot of effort into a SmartPoopCon 2030 that is out of the ordinary.

> Good evening to all and thank you for coming in such large numbers! Are you all well? Are you ready to make history?

At these words, the audience goes wild as if their soccer team had just scored a goal.

> I'd like to start this conference with a number: 100,000. That's the goal we set two years ago when we launched Poo. We hoped to divide by 10 the number of suicides in the world in five years, to bring it down from its historical figure of 1 million following ROVID-19 in 2022[^suicides], to only 100,000. A goal considered unrealistic by many, including the newspaper *The Warden*. Where are we today, in 2030?

[^suicides]: In 2020, it was estimated that there were around 800,000 suicides per year worldwide.  
[**Web.** Suicide. Our World in Data (2020).](https://ourworldindata.org/suicide)  
Among the countries most affected by these suicides are low, medium and highly developed countries such as Suriname, Russia and South Korea. Essentially all developed countries have a very high suicide rate, often with more than one suicide per 10,000 people.  
[**Web.** Suicide death rates. Our World in Data (2019).](https://ourworldindata.org/grapher/suicide-mortality-rate)

Katia marks a silence.

> Let's have a countdown to find this out.

The musical jingle takes over from Katia's speech, and concludes with a countdown, taken up in chorus by a boiling audience. Three. Two. One. At zero, on the giant screen behind Katia, the figure of 97 643 is displayed on the screen!

> We did it !

The audience celebrates the number as if they were supporting a soccer team that had just scored the winning goal in the last minute. The audience's applause then begins to build to a rhythm, and lasts for a full minute.

> Poo, our algorithmic psychiatrist, now accompanies billions of us in our mental problems. And it's not only suicides that Poo has managed to fight. Let me show you some other curves, which have been validated by different external auditors, thanks to the coordination of WAISO. Ladies and gentlemen, this is the percentage of happy communications with Poo over time.

A curve appears on the screen, drawn smoothly from left to right. This curve starts at 37%, and keeps increasing over time, until it reaches the figure of 67%, under the cheers of a wild audience.

> 67% !! Incredible! Poo has made all humanity happier[^emotional-contagion]!

[^emotional-contagion]: In 2014, a publication by Facebook and academic co-authors showed that a very slight reduction in the publication, in news feeds, of posts with negative emotions, leads users exposed to these posts to write happier posts.  
[**Podcast.** Can Algorithms Choose our Emotions? Robustly Beneficial (2020).](https://www.youtube.com/watch?v=gQHvTow91FY)  
[**Paper.** Experimental evidence of massive-scale emotional contagion through social networks. Adam Kramer, Jamie Guillory & Jeffrey Hancock. PNAS (2014).](https://www.pnas.org/content/111/24/8788)  
Conversely, the *facebook files* reveal that Facebook's internal research, kept secret, shows that the user engagement-maximizing algorithms that were rolled out in 2018 by Facebook led to far more anger and insults.  
[**Podcast.** The Facebook Files, Part 4: The Outrage Algorithm. The Journal (2021).](https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-4-the-outrage-algorithm/e619fbb7-43b0-485b-877f-18a98ffa773f)

The audience expresses this joy, by the way, at the sight of this curve and this number.

> When you talk to Poo, you can talk about yourself, your well-being and your problems, which I'll call *egocentric* conversations. Or you can talk about other people, the joy they bring you, the difficulties they're going through and the things you can do to help them. I'm going to call that *allocentric* conversations. Before Poo launched, 57% of Poo discussions were egocentric rather than allocentric. How do you think that number has evolved? Upwards?

The audience then shouts "no" in chorus.

> Down?

The audience shouts in chorus "yes".

> Let's find out!

On the screen, the same animation as before shows a curve that goes down, until it reaches 44%, under the ovations of the public.

> Yes! 44%. Now most discussions with Poo are discussions about the social entourage rather than about oneself. And so, we should be wary of this figure a priori. Initially, most allocentric discussions consisted of complaining about others, mocking certain groups, and even *dog-piling* against certain individuals[^dog-piling], rather than celebrating others, rejoicing in their successes, and thinking about how to help them. Before Poo launched, 86% of allocentric discussions were critical, not caring. How has that statistic changed?

[^dog-piling]: On social medias, dog-piling typically occurs when an influencer asks their community to attack a target individual or group.  
[**Paper.** When Online Harassment Is Perceived as Justified. Lindsay Blackwell, Tianying Chen, Sarita Schoenebeck & Cliff Lampe. ICWSM (2018).](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewFile/17902/16993)

The audience then shouts in a disorganized "down" manner. Then with repetition, the shouts become synchronized, before repeating "down", "down", "down".

> Let's see.

The giant screen shows the evolution of this curve, which indeed plunges downwards, until it reaches 47%, under the applause of the public.

> Incredible! Society has become incredibly more benevolent and altruistic in just two years! In fact, the expression "the poorest" is used today 3 times more often than two years ago, while the expression "future generations" is used 4 times more often. And according to many psychiatrists, this increase is most likely directly related to Poo, and to the improved mental health of our users. When you feel better yourself, you are immediately much more likely to wish others happiness and to help them[^altruism-happiness]!

[^altruism-happiness]: Many studies seem to show a significant association between altruism and happiness. Intriguingly, it seems in particular that being altruistic increases happiness, especially as opposed to spending our money on ourselves.  
[**Paper.** Altruism, happiness, and health: it's good to be good. Stephen Post. International Journal of Behavioral Medicine (2005).](https://link.springer.com/article/10.1207/s15327558ijbm1202_4)    
[**Video.** Helping others makes us happier -- but it matters how we do it | Elizabeth Dunn. TED (2019).](https://www.youtube.com/watch?v=lUKhMUZnLuw)  
[**Book.** Happy Money: The Science of Smarter Spending. Elizabeth Dunn and Michael Norton. Simon & Schuster (2013).](https://www.simonandschuster.com/books/Happy-Money/Elizabeth-Dunn/9781451665079)

### The Heroes Behind Poo

> So what do you think? Poo, success or failure?

The public shouts "success" in a haphazard, yet distinguishable way.

> Well, ladies and gentlemen, I think that Poo is not a failure.

Katia pauses, while the audience applauds.

> And that's thanks to the great work of so many enthusiasts, so many people who have given so much of their time and money to make Poo better and safer. Really, I'm literally just a pretty face here, which is supposed to represent the monumental collaborative work of hundreds of thousands of world-class heroes — well I don't know about pretty face, but you see what I mean.

The audience laughs.

> I have a huge thought of course for all my colleagues at SmartPoop, and their incredible devotion. But they are not the only ones responsible for SmartPoop. Poo was built through close, daily collaboration with thousands of psychiatrists and psychologists around the world, and through the discussion data of millions of volunteer psychiatrists and patients. Nothing would have been possible without them[^data-experts].

[^data-experts]: It is worth remembering that machine learning algorithms *learn from data*. They will only be able to perform difficult tasks, such as providing therapeutic support for their users' mental health, if these algorithms have a huge amount of reliable and secure data that allows them to understand how to perform these tasks.  
[**Video.** Data is manipulating the algorithms. Science4All (2021).](https://www.youtube.com/watch?v=Ce-FvIvt27I&list=PL8ovs-QtxcNxcwlsTF5O9NXtr3NAj_SVc&index=3)

Katia takes advantage of the audience's applause to take another break.

> But that's not all. Poo is the product of the whole human civilization. In particular, nothing would have been possible without the agreements between the great world powers, which allowed the creation of WAISO, and the planetary coordination of the research on the ethics and security of algorithms. So I also thank all the scientists around the world who have given up their quests for performance, or sometimes their quests for mathematical elegance, to take up the challenge of ethics and security[^quete-performance]. But more than that, WAISO has often served as a counterweight to the quest for military and economic power by governments and corporations. Without them, today's most influential algorithms would be cyber-warfare malware and algorithms optimized to grab users' attention by promoting sensationalist putaclic. Personally, I think that every member and volunteer of WAISO has saved humanity.

[^quete-performance]: In 2021, academic research (and even more so industry research) is still largely obsessed with the quest for "impressive" performance or results, both in machine learning and in computer science in general, relying on metrics such as *accuracy* (predictive performance on a "classical" dataset), computation time, *throughput* (the amount of information transmitted) or latency. While algorithms already have monumental side-effects on a global scale, this research seems to aggravate the race for performance, and thus the hasty deployment of poorly tested and rarely audited technologies.  
As an example, here is a comment from an anonymous NeurIPS 2019 *reviewer*, following a paper submitted by Lê Nguyên Hoang and his co-authors on an algorithm to debiased racial bias in algorithms: "Unfortunately, I don't think the problem the authors introduce is one that has value to the academic community or to ML practitioners. Given this, I can't recommend the paper for publication."  
[**Paper.** Removing Algorithmic Discrimination (With Minimal Individual Error). El Mahdi El Mhamdi, Rachid Guerraoui, Lê Nguyên Hoang & Alexandre Maurer (2018).](https://arxiv.org/abs/1806.02510)
That being said, though arguably largely insufficient still, there have been a few recent positive advances, such as the creation of the Fairness, Accountability and Transparency (FAccT) conference, the introduction of *ethical guidelines* in these conferences, or the addition of a mandatory discussion by authors in their papers of the societal impacts of their research.  
[**Video.** AI Ethics under Major Threat. Science4All (2020).](https://www.youtube.com/watch?v=HTzEkz8sFEE)

Again, Katia pauses during the audience's applause.

> Last but not least, I would like to thank each of the actors and signatories of the open letter, as well as each of the journalists and influencers who covered this case, thanks to whom SmartPoop got back on track. These women and men risked their lives, their individual well-being and safety, to help the many[^whistleblower]. Without them, who knows what SmartPoop would have become? Who knows what I would have become?

[^whistleblower]: Unfortunately, whistleblowers often suffer more mental health issues as a result of their courageous actions. Given the critical role they play in exposing scandals in dangerously opaque companies and organizations, it seems urgent that they be much better supported.  
[**Paper.** Mental Health Problems Among Whistleblowers: A Comparative Study. Peter van der Velden, Mauro Pecoraro, Mijke Houwerzijl & Erik van der Meulen. Psychological Reports (2018).](https://journals.sagepub.com/doi/10.1177/0033294118757681)

Alone on the stage, Katia bursts into tears.

> Thank you to these heroes, she says while crying.

The audience applauds this very moving moment.

### The challenge of the ethics of Poo

Katia still needs several seconds to recover her emotions. Finally, she carries on her speech.

> Nevertheless, I refuse to say that Poo is a success. Together, we have created an incredible product. But it is still infinitely improvable, especially in terms of its ethics, security and governance. How do you control Poo? How do you stop Poo from saying hurtful words, revealing secrets, repeating hate speech, and spreading misinformation[^language-model]? How do we get Poo to be consistently kind to his interlocutors, to say the right words to make them feel better and to promote as much as possible reliable and non-misleading information[^non-misleading]? How do we make Poo want to investigate its uncertainties rather than being satisfied with its intuitions[^scout]? More importantly, how can we collectively decide what Poo should say? How do we determine what is desirable to say, and what should never be said[^ethics-of-influence]?

[^language-model]: These problems are completely unsolved for modern conversational algorithms, which are highly vulnerable to espionage or data poisoning attacks. And yet, these algorithms are already deployed on a very large scale, via smart keyboards, via personal assistants (Siri, Alexa, OK Google) and via search engines (Google, YouTube).  
[**Paper.** On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?  Emily Bender, Timnit Gebru, Angelina McMillan-Major & Shmargaret Shmitchell. FAccT (2021).](https://dl.acm.org/doi/10.1145/3442188.3445922)  
[**Paper.** The Radicalization Risks of GPT-3 and Advanced Neural Language Models. Kris McGuffie, Alex Newhouse (2020).](https://arxiv.org/abs/2009.06807)  
[**Paper.** Extracting Training Data from Large Language Models. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea & Colin Raffel. USENIX (2021).](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)

[^non-misleading]: [**Video.** What’s a message of public utility? Science4All (2021).](https://www.youtube.com/watch?v=NAOlLB9VjJ4&list=PL8ovs-QtxcNxcwlsTF5O9NXtr3NAj_SVc&index=5)

[^scout]: Julia Galef talks about the *scout mindset*, as opposed to the *soldier mindset*. According to her, adopting the scout mindset is the most important step towards analyzing information more correctly.  
[**Video.** Why "scout mindset" is crucial to good judgment | Julia Galef | TEDxPSU (2016).](https://www.youtube.com/watch?v=3MYEtQ5Zdn8)  
[**Book.** The Scout Mindset: Why Some People See Things Clearly and Others Don't. Julia Galef. Penguin (2021).](https://www.penguinrandomhouse.com/books/555240/the-scout-mindset-by-julia-galef/)

[^ethics-of-influence]: Every word chosen or every recommendation made by an algorithm can be seen as a *nudge*. Many studies show that the acceptability and effectiveness of *nudges* depend strongly on the *nudge* considered; with more data on this topic, it might thus be possible to implement particularly socially accepted and effective nudges.  
[**Book.** The Ethics of Influence: Government in the Age of Behavioral Science. Cass Sunstein. Cambridge University Press (2016).](https://www.cambridge.org/core/books/ethics-of-influence/E29EDE19EBCB53F6D8691730668115F7)

Katia pauses, in front of an attentive audience.

> And that brings me to the famous rumor you've probably heard about. Supposedly, we would have a plan to solve the problem of the ethics of the information...

Katia then marks a silence, before adding in a sarcastic tone.

> Like, "SmartPoop, this shitty application, is going to solve ethics".

The audience laughs heartily.

> You know, you shouldn't believe everything you're told.

Katia marks another silence.

> But in this case, yes it's true. Or at least, we intend to contribute to it.

The public laughs.

> And, I know it well, because I'm the one who leaked the rumour.

The public, won over, is laughing again, even though many are starting to get a confused look on their faces.

> At SmartPoop, we are deeply committed to ethics of information. We want to figure out what information should be collected by whom and under what conditions, how it should be stored, to whom this information should be accessible, what processing of this information should be done, how these information processes should be audited and secured, where to store the results of these calculations, who can have access to these results, and who will be notified of the existence of these results.

After another silence, Katia resumes her speech.

> And now, I feel that there are quite a few of you who are saying: "But who does Katia think she is?".

The public laughs again.

> Of course, many of these operations should be largely configurable by the users. That said, most users will not want to manage all the configurations of all their information systems, and check, for example, that those configurations comply with data privacy regulations, or prevent the mass distribution of hate speech. No, most users are like me: they are lazy.

The laughter of the audience allows Katia to catch her breath.

> But above all, on the Internet, many users actually want to harm other users; or at least influence them in some way. Think of all the disinformation campaigns that are rampant on social medias[^disinformation-2]. Even one of the greatest defenders of liberalism, the philosopher John Stuart Mill, believed that the freedom of some must stop where it harms others. This is the *no-harm principle*.

[^disinformation-2]: [**Video.** Social medias are dangerous. Very dangerous. Science4All (2021).](https://www.youtube.com/watch?v=w5JRKUndWNk&list=PL8ovs-QtxcNxcwlsTF5O9NXtr3NAj_SVc&index=1)

Katia pauses again, and looks at her audience, shifting her gaze from left to right.

> One of the great challenges of the ethics of information is the implementation of the no-harm principle. For as far as I know, this principle is quasi-consensual in moral philosophy[^absence-tort] - and it is quite a feat to be consensual in moral philosophy.

[^absence-tort]: [**Video.** The Harm Principle: How to live your life the way you want to. BBC Radio 4 (2014).](https://www.youtube.com/watch?v=R9IM3ZKNMCk)

Katia marks a new silence.

> Well, that's in principle. In practice, it is extremely difficult to agree on what constitutes a wrong. Does an aggressive comment cause harm? Does a little lie cause harm? Does lying by omission cause harm? Does a joke at the expense of a community cause harm? Unfortunately, in practice, we won't agree. We have ethical preferences that are difficult to reconcile, and sometimes clearly irreconcilable. So what to do?

Katia really seems to be asking this question to her audience, as if she is waiting for an answer. The audience looks thoughtful, and eagerly awaits an answer from Katia.

### Who will decide about the Poo ethics ?

Katia raises another question.

> Should SmartPoop decide?

The public remains silent. Clearly, it is a group of SmartPoop fans. But even they don't seem to be thrilled with the idea.

> If you ask me, the answer is clearly no. We saw that two years ago. A structure like SmartPoop can lose its ethics and its way. And even though we have made a lot of progress in our governance to prevent this from happening again, I don't think SmartPoop is robust enough for such a task. Even I don't sufficiently trust SmartPoop with the future of all of humanity.

Katia pauses, as if seriously inviting the audience to think about it.

> But then, who? Who should determine the ethics of information, of how it's produced, stored, moderated and spread?

Once again, this question is asked as if Katia had no answer to it, and as if she expected the public to answer. After long seconds, Katia offers her answer.

> Well, SmartPoop's proposal is to let *you* decide. Or rather, all of us. Together, all of humanity should collectively decide on the ethics of information.

The audience applauds.

> But... How do you get billions of people to decide collectively on something as complex as information ethics?

The audience is now circumspect.

> Think about it. How do we make collective decisions today?

Katia pauses again, until she hears someone in the audience shouting "the vote".

> The vote, yes! In many countries around the world, when a collective decision has to be taken, they often try to reduce it to a question with a yes or no answer, and they ask the people to vote for yes, or for no? This is how irreconcilable disagreements are settled. In fact, when you think about this, it's actually absolutely remarkable that in democracies throughout the world, we have agreed on how to agree on what to do, even when we agree that we will never actually agree on what ought to be done[^meta-ethics]!

[^meta-ethics]: Technically, this corresponds to finding a consensus on meta-ethics rather than on ethics itself. The hope is essentially that we are more likely to agree on meta-ethics than on ethics. In fact, even meta ethics considerations are likely to remain polarized, such as how to allocate voting rights, in which case meta-meta-ethics may be needed to find agreement.

Katia stops for a few seconds.

> But the problem with voting, at least as it is practiced today, is that it only allows each citizen to send a few bits of information per vote. COP 30, for or against? Mandatory vaccination, for or against? Regulation of algorithms, for or against? Which of the following 15 candidates should be elected? There are not 36 000 possible answers to these questions. And yet, to solve the ethics of algorithms, complex answers will have to be provided. There are billions of billions of speeches that a SmartPoop user can produce. Among the billions of imaginable answers, which one will Poo have to adopt?

Katia catches her breath, before the rest of her speech.

> What if we now designed votes where everyone's voice was not reduced to a binary[^geometric-median] answer, given only once a year? What if we allowed everyone to share the full complexity of their ethical judgment? What if we managed to take into account all this complexity in order to collaboratively decide the ethics of information[^licchavi]?

[^geometric-median]: One solution to high-dimensional voting is to rely on the "one voter, one unitary force" principle, which can typically lead to using the *geometric median*.  
[**Paper.** On the Strategyproofness of the Geometric Median. El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui & Lê-Nguyên Hoang (2021).](https://arxiv.org/abs/2106.02394)

[^licchavi]: This will certainly require combining ballot systems with learning methods. This is what Licchavi proposes.  
[**Paper.** Strategyproof Learning: Building Trustworthy User-Generated Datasets. Sadegh Farhadkhani, Rachid Guerraoui & Lê-Nguyên Hoang (2021).](https://arxiv.org/abs/2106.02398)

Katia marks a new pause.

### The fabulous construction site[^fabulous-endeavor]

[^fabulous-endeavor]: Le fabuleux chantier is the name of a previous book by Lê Nguyên Hoang, one of the authors of this book.  
[**Book.**  Le fabuleux chantier : Rendre l'intelligence artificielle robustement bénéfique. Lê Nguyên Hoang et El Mahdi El Mhamdi. EDP Sciences (2019). *English translation pending*.](https://laboutique.edpsciences.fr/produit/1107/9782759824304/Le%20fabuleux%20chantier)

Katia then changes her tone, taking a deeper and more composed voice.

> Ladies and gentlemen, today is a historic day, because I am going to present you the result of two years of work, in intimate collaboration with WAISO and many other groups and academic scholars. As I speak, a new platform has just gone live, called girasol.app[^girasol-tournesol]. Girasol is an entirely open source website, under a free license[^license], that will coordinate the design of information ethics, allowing users to provide ethical judgments, and using voting algorithms to collaboratively build an ethics of information from users' ethical judgments! The first stage of a democratic ethics of information has been laid!

[^girasol-tournesol]: Girasol does not exist, but it is in fact clearly a reference to the [tournesol.app project](https://tournesol.app) launched by Lê Nguyên Hoang, one of the authors of this book. The rest of the book actually describes the global vision of Tournesol. You can find much more information on the [Tournesol wiki](https://wiki.tournesol.app). It is worth noting that, especially for now, the purpose of Tournesol is more to serve as a "microscope of human judgments", i.e., a tool for collecting data on what humans find ethically preferable. In particular, and among other things, Tournesol hopes to detect moral consensuses that are currently difficult to observe due to lack of data. It also hopes to motivate more research on the ethics of information.

[^license]: Tournesol's code is under [AGPL license](https://www.gnu.org/licenses/agpl-3.0.en.html), while the public database is under [ODbL license](https://opendatacommons.org/licenses/odbl/) (to be confirmed).

The public applauds this announcement by Katia.

> I would like to point out that the governance of this project is entirely under the control of WAISO today, and SmartPoop only serves, and will serve, as a volunteer contributor to the code base and promotion of the project. All code is audited by many entities, so it is almost impossible for the project to be hijacked by an evil entity - and that includes potential SmartPoop investors!

Katia pauses again.

> So, there are many other important details to clarify about this complex project. But you should know that, in collaboration with WAISO, we've done our best to make each of these details a research topic that different multidisciplinary teams around the world are working on. These details include issues such as authenticating accounts, avoiding fake accounts, exploiting *Proof of Personhood* mechanisms, and ensuring that each authenticated account has the same voting rights as any other authenticated account. They also include identifying user expertise and overconfidence, to avoid anti-scientific theories polluting the ethics of information. We can also mention the individual customization of the platform, so that this individual exploits the most appropriate way for him to express his ethical judgments. Or the algorithms for rectifying the participation bias, to properly take into account the preferences of those who could not participate in Girasol, due to lack of time or Internet access[^defis-tournesol].

[^defis-tournesol]: The Tournesol project raises a lot of challenges, from research to development, promotion, funding, and partnerships, among others, that Tournesol members will absolutely not be able to solve alone. *You* can help. To learn more, especially about the research and development aspect, we encourage you to read the project's technical *white paper*.  
[**Paper.** Tournesol: A quest for a large, secure and trustworthy database of reliable human judgments. Lê-Nguyên Hoang, Louis Faucon, Aidan Jungo, Sergei Volodin, Dalia Papuc, Orfeas Liossatos, Ben Crulis, Mariame Tighanimine, Isabela Constantin, Anastasiia Kucherenko, Alexandre Maurer, Felix Grimberg, Vlad Nitu, Chris Vossen, Sébastien Rouault & El-Mahdi El-Mhamdi (2021).](https://arxiv.org/abs/2107.07334)

Katia stops for a few moments.

> In short. There are many remarkable research challenges that Girasol will have to solve, to then allow for an adequate collaborative design of an ethics of information. Girasol is clearly a monumental and extremely challenging task. And it is also an urgent task to solve.

Katia takes a breath to conclude her speech.

> But above all, Girasol is a *fabulous* endeavor. If you ask me, it is for me the most fabulous of all the endeavors ever carried out by humanity, even more grandiose than building pyramids, more ambitious than eradicating pandemics like smallpox[^pox], and more earth-changing than sending humans to the Moon. Girasol is about uniting all of humanity behind the most important aspect of human civilization: the collaborative mastery of information[^next-book], and ensuring that it flows as we, humanity, would really like it to flow, when we ponder it thoroughly, with benevolence and rigor. Ladies and gentlemen, together let's solve the information ethics[^solve-ethics]!

[^pox]: [**Video.** Humanity's greatest triumph. Science4All (2020).](https://www.youtube.com/watch?v=eAzP2QtAAag&list=PLtzmb84AoqRS0SN8VKvAxuGOdcINPRugV&index=9)

[^next-book]: This could actually be the subject of a future book by Lê Nguyên Hoang... #teaser

[^solve-ethics]: [**Video.** Let's solve ethics collaboratively!! Science4All (2021).](https://www.youtube.com/watch?v=J-0zvNL6O3k&list=PL8ovs-QtxcNxcwlsTF5O9NXtr3NAj_SVc&index=4)

At these words, the public explodes with joy and enthusiasm, while the applauds gradually give way to Katia's name, which is rhythmically chanted by the whole stadium. Alone on stage, Katia enjoys the moment, with a radiant smile, and waves to the audience. At this moment, she thinks of all that SmartPoop has accomplished so far. But also and above all, Katia is incredibly excited by the vision of a human civilization that, thanks to Girasol, will finally take the fate of its civilization into its own hands.

Collaboratively.


## To go further

This is it, the novel is finished!
You can go back to the [outline](README.md).  
If you enjoyed it, please consider sharing and promoting this science fiction novel to your friends.
We would be very grateful!
